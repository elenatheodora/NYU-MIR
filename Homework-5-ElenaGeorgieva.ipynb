{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "**Total Points: 5**\n",
    "\n",
    "**Instructions:**\n",
    "1. Complete parts 1 through 5, filling in code or responses where marked with `# YOUR CODE HERE` or `# YOUR ANALYSIS HERE`.\n",
    "2. The libraries you need, in the order you need them, have already been coded. Do not import additional libraries or move import commands.\n",
    "3. When finished, run the full notebook by selecting <b>Kernel > Restart & Run All</b>. </li>\n",
    "4. Submit this completed notebook file to <b>NYU Classes</b>. </li>\n",
    "\n",
    "In this assignment you will create a Nearest Neighbor classifier, train it, and test it. The goal of your classifier to predict whether the input audio contains a \"talking\" voice or a \"singing\" voice based on the training data. This assignment contains a subfolder called `audio` which has multiple short audio files from the dataset *VocalSet: A Singing Voice Dataset*.\n",
    "\n",
    "**Grading:** Each part is worth 1 point.\n",
    "\n",
    "**Important Note**: The way you implement the code in your work for each assignment is entirely up to you. There are often many ways to solve a particular problem, so use whatever method works for you, including testing different parameter values. The only requirement is that you follow the instructions, which may prohibit or require certain libraries or commands.\n",
    "\n",
    "Refer to https://scikit-learn.org/ for implementation instructions and tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from librosa import feature\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import linalg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Prepare Data\n",
    "Create a function `prepare_data()` that will take an array of files and prepare them for a nearest neighbor machine learning task. This function should:\n",
    "1. Take the input array of audio files for one class (arrays are provided in Part 2),\n",
    "2. Concatenate the audio into one long audio file,\n",
    "3. Generate an `mfccs` matrix from this one audio file (use `Librosa.feature.mfcc()` for this),\n",
    "4. Remove the first MFCC.\n",
    "5. Generate a `label` that is an array of the label number the same size as the number of samples (each set of MFCCs should have the same corresponding label),\n",
    "6. Output both `mfccs` and `labels` from the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(audiofiles, label):\n",
    "    \n",
    "    \"\"\" Prepare data for Nearest Neighbor classification\n",
    "        Hint: When generating MFCCs, you can use Librosa's default\n",
    "        or experiment. 13 is a common number of MFCC coefficients to retain.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    audiofiles: np.array\n",
    "        array of input audio files\n",
    "    \n",
    "    label: int\n",
    "        The label to give these audio features\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    mfccs: np.array\n",
    "        mfcc feature set\n",
    "    labels: np.array\n",
    "        labels for feature set\n",
    "        \n",
    "    \"\"\"\n",
    "    fs = 44100 # sampling rate of files \n",
    "    \n",
    "    # Concatenate the audio into one long audio file,\n",
    "    long_audio = []\n",
    "    y1, fs = librosa.load(audiofiles[0])\n",
    "    y2, fs = librosa.load(audiofiles[1])\n",
    "    y3, fs = librosa.load(audiofiles[2])\n",
    "    y4, fs = librosa.load(audiofiles[3]) \n",
    "\n",
    "    long_audio = np.concatenate((y1,y2,y3,y4))\n",
    "\n",
    "    # Generate an `mfccs` matrix from this one audio file use Librosa.feature.mfcc()\n",
    "    mfccs = librosa.feature.mfcc(long_audio, fs) #default 20 mfccs should i change to 13?\n",
    "\n",
    "    # Remove the first MFCC.\n",
    "    mfccs = mfccs[1:len(mfccs)] # 2241 by 19\n",
    "    mfccs = mfccs.T\n",
    "    \n",
    "    # Generate a `label` that is an array of the label number the same size as the number of samples (each set of MFCCs should have the same corresponding label)\n",
    "    labels = np.empty([len(mfccs),1]) # 2241 by 1\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = label\n",
    "    \n",
    "    # output both returns\n",
    "    return mfccs, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Set Up the Experiment\n",
    "\n",
    "Using `prepare_data()` prepare your Nearest Neighbor classification experiment as follows:\n",
    "\n",
    "- Run `prepare_data()` twice, once using the files in array `talking_files` and once using `singing_files`,\n",
    "- Append the feature vectors and label vectors so that you have two data objects; one with all features and one with all labels.\n",
    "- Take care that the objects' dimensions match and that `label[n]` corresponds to `feature_vector[n]`.\n",
    "\n",
    "**Hint:** Rows should be samples, and columns should be features. Python is row-centric, so rows come first. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use singing_files and talking_files for Part 2; the vibrato_files is for Part 5\n",
    "singing_files = np.array(['audio/f1_row_straight.wav','audio/f2_row_straight.wav','audio/m1_row_straight.wav','audio/m2_row_straight.wav'])\n",
    "talking_files = np.array(['audio/f1_row_spoken.wav','audio/f2_row_spoken.wav','audio/m1_row_spoken.wav','audio/m2_row_spoken.wav'])\n",
    "vibrato_files = np.array(['audio/f1_row_vibrato.wav','audio/f2_row_vibrato.wav','audio/m1_row_vibrato.wav','audio/m2_row_vibrato.wav'])\n",
    "\n",
    "# Run prepare_data() twice: once using the files in array talking_files and once using singing_files:\n",
    "talk_mfccs, talk_labels = prepare_data(talking_files, 0) # 0 is label for talking\n",
    "sing_mfccs, sing_labels = prepare_data(singing_files, 1) # 1 is label for singing\n",
    "\n",
    "# Append the feature vectors and label vectors so that you have 2 data objects; one with all features and one with all labels.\n",
    "all_mfccs = np.concatenate((talk_mfccs, sing_mfccs))\n",
    "all_labels = np.concatenate((talk_labels, sing_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Find the Nearest Neighbor\n",
    "Create a function `find_nearest_neighbor()` without using scikit-learn (except for the function in step #1), You may use a process of your choice. One way to write the code is as follows:\n",
    "\n",
    "1. Separate your data into training and testing sets (use scikit-learn's `train_test_split()` for this - start with a test_size of 0.1 (10%).\n",
    "2. For the first feature vector in the test set, find the Euclidean distance between it and every vector in the training set, keeping track of the distances in an array.\n",
    "3. When finished, use `np.argmin()` to find the index of the smallest distance. That is the Nearest Neighbor of that first test feature vector.\n",
    "4. If the label of the training vector with the lowest distance is the same as the label of the test vector, then it is a match. Otherwise it is wrong.\n",
    "5. Repeat steps 2-4 for every test vector.\n",
    "8. When finished, the program should output the number of correct vs incorrect matches.\n",
    "9. Run the program multiple times, making sure that the test/training sets are random each time. You can also adjust the size of the test set.\n",
    "\n",
    "**Note that this implementation will take a while to run.** Also note that, since this function is only finding the nearest neighbor (and not k-neighbors), there is no validation set.\n",
    "\n",
    "\n",
    "The equation for finding the Euclidean distance between two N-dimentional vectors $p$ and $q$ is:\n",
    "\n",
    "$$d(p,q) = \\sqrt{ \\sum_{n=0}^{N-1}{ (p_n - q_n)^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbor(data, labels, test_size):\n",
    "    \n",
    "    \"\"\" Perform Nearest Neighbor classification    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.array\n",
    "        feature set\n",
    "    \n",
    "    labels: np.array\n",
    "        true labels for the features\n",
    "        \n",
    "    test_size: float\n",
    "        Between 0 and 1, the amount of data set aside for testing\n",
    "    Returns\n",
    "    -------    \n",
    "    [correct, incorrect]: np.array\n",
    "        the numer of correct and incorrect results\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Separate your data into training and testing sets (use scikit-learn's train_test_split() for this - start with a test_size of 0.1 (10%).\n",
    "    data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=test_size)\n",
    "    \n",
    "    # 2. For the first feature vector in the test set, find the Euclidean distance between it and every vector in the training set, keeping track of the distances in an array.\n",
    "    correct_matches = 0 # counters\n",
    "    incorrect_matches = 0 \n",
    "    \n",
    "    ntest_iterator = 0 # iterator to ocount which test vector we are at. There's probably a better way to do this.\n",
    "    for ntest in data_test[:,:]: # For each test vector\n",
    "        dist_arr = [] \n",
    "        for ntrain in data_train[:,:]: # for each train vector\n",
    "            dist_arr.append(np.linalg.norm(ntest - ntrain)) # distances between 1 test vector and all training vectors.\n",
    "        \n",
    "        # 3. When finished, use np.argmin() to find the index of the smallest distance. \n",
    "        index = np.argmin(dist_arr) # index is the Nearest Neighbor of the ntest feature vector\n",
    "\n",
    "        # 4. If the label of the training vector with the lowest distance is the same as the label of the ntest test vector, then it is a match. Otherwise wrong.\n",
    "        if (labels_train[index] == labels_test[ntest_iterator]):\n",
    "            correct_matches += 1\n",
    "        else:\n",
    "            incorrect_matches +=1\n",
    "        ntest_iterator += 1 # goes to next test vector\n",
    "\n",
    "    #6. Program outputs the number of correct vs incorrect matches.\n",
    "    end = [correct_matches, incorrect_matches]\n",
    "    print(\"find_nearest_neighbor results: \", end)\n",
    "    return end\n",
    "\n",
    "    # Run the program multiple times, making sure that the test/training sets are random each time. You can also adjust the size of the test set.\n",
    "    \n",
    "    ## I ran the program multiple times, getting scores of [464, 29], [466, 27], and [463, 30] which are 94%, 94.5%, 93.9% respectively. The test/train sets are random each time. The average accuracy is 94.1%\n",
    "    ## Then I changed to an 80-20 train-test set and got [939, 46], [937, 48], and [944, 41] which is slightly better but not significantly! It's average accuracy is 95.3%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Cross Validation and Confusion Matrix\n",
    "Using Scikit-Learn, set up a Nearest Neighbor experiment using k-fold cross validation and output a confusion matrix. All the sklearn modules you need have been imported for you.\n",
    "1. Break the data into training and testing sets (as before).\n",
    "2. Create a new classifier by using neighbors.KNeighborsClassifier()\n",
    "3. \"Fit\" the training data to the model (this basically means \"train the model\").\n",
    "4. Use `cross_val_scores()` on the classifier, setting cv=10 and output the scores.\n",
    "5. Get predictions using `predict() with the test data.\n",
    "6. Print the confusion matrix using the true vs. predicted labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      " [[192  13]\n",
      " [ 12 276]]\n",
      "Correct matches: 468\n",
      "Inorrect matches: 25\n"
     ]
    }
   ],
   "source": [
    "# YOU CAN USE THESE LIBRARIES\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#1 Break the data into training and testing sets (as before).\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(all_mfccs, all_labels, test_size=.1)\n",
    "\n",
    "#2 Create a new classifier by using neighbors.KNeighborsClassifier()\n",
    "neighbors = neighbors.KNeighborsClassifier() #defaul nneighbors i 5\n",
    "\n",
    "#3 \"Fit\" the training data to the model (this basically means \"train the model\").\n",
    "thefit = neighbors.fit(data_train, labels_train.ravel())\n",
    "\n",
    "#4 Use cross_val_scores() on the classifier, setting cv=10 and output the scores.\n",
    "cross_val_score(thefit, data_train, labels_train.ravel(), cv=10)\n",
    "\n",
    "#5 Get predictions using `predict() with the test data.\n",
    "final_predictions = neighbors.predict(data_test)\n",
    "\n",
    "#6 Print the confusion matrix using the true vs. predicted labels.\n",
    "c = confusion_matrix(labels_test, final_predictions)\n",
    "print(\"confusion matrix:\\n\" , c)\n",
    "print(\"Correct matches:\" , c[0,0] + c[1,1])\n",
    "print(\"Inorrect matches:\" , c[1,0] + c[0,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Analysis\n",
    "How would you characterize your overall results? Does your classifier work better or worse than `sklearn`? Or are they similar?\n",
    "\n",
    "There is a third class of audio included with this assignment, `vibrato_files`,  which contains singers using vibrato. Add this data to your full dataset and give it a unique label. Then run the whole test again (using `sklearn` - no need to run your code from Part 3 again). \n",
    "1. Perform 10-fold cross validation again. How so the results compare now that you have 3 classes.\n",
    "2. Generate a confusion matrix again. How does that data compare?\n",
    "\n",
    "(OPTIONAL) MFCC deltas ($\\Delta$) may (or may not) improve the overall results. In addition to using the MFCCs generated by the MFCC function, append to each vector an MFCC difference such that, for each MFCC feature vector $f[m]$ at time index $m$, $\\Delta f[m] = f[m] - f[m-1]$. This will double the amount of features per vector. Report on your findings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[correct_matches, incorrect_matches]:  [467, 26]\n",
      "confusion matrix:\n",
      " [[194   9   5]\n",
      " [ 12 231  16]\n",
      " [  6  28 257]]\n",
      "Correct matches: 682\n",
      "Inorrect matches: 76\n"
     ]
    }
   ],
   "source": [
    "# YOU CAN USE THESE LIBRARIES\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "find_nearest_neighbor(all_mfccs, all_labels, 0.1) # Running code for part 3!!! \n",
    "\n",
    "# Vibrato_files\n",
    "vibrato_mfccs, vibrato_labels = prepare_data(vibrato_files, 2) # 2 is label for vibato\n",
    "all_mfccs_w_vibrato = np.concatenate((talk_mfccs, sing_mfccs, vibrato_mfccs))\n",
    "all_labels_w_vibrato = np.concatenate((talk_labels, sing_labels, vibrato_labels))\n",
    "\n",
    "#1 Break the data into training and testing sets (as before).\n",
    "data_train1, data_test1, labels_train1, labels_test1 = train_test_split(all_mfccs_w_vibrato, all_labels_w_vibrato, test_size=.1)\n",
    "\n",
    "#2 Create a new classifier by using neighbors.KNeighborsClassifier()\n",
    "neighbors_vibrato = neighbors.KNeighborsClassifier() #defaul nneighbors i 5\n",
    "\n",
    "#3 \"Fit\" the training data to the model (this basically means \"train the model\").\n",
    "thefit_vibrato = neighbors_vibrato.fit(data_train1, labels_train1.ravel())\n",
    "\n",
    "#4 Use cross_val_scores() on the classifier, setting cv=10 and output the scores.\n",
    "cross_val_score(thefit_vibrato, data_train1, labels_train1.ravel(), cv=10)\n",
    "\n",
    "#5 Get predictions using `predict() with the test data.\n",
    "final_predictions_vibrato = neighbors_vibrato.predict(data_test1)\n",
    "\n",
    "#6 Print the confusion matrix using the true vs. predicted labels.\n",
    "c = confusion_matrix(labels_test1, final_predictions_vibrato)\n",
    "print(\"confusion matrix:\\n\", c)\n",
    "print(\"Correct matches:\" , c[0,0] + c[1,1]+ c[2,2])\n",
    "print(\"Inorrect matches:\" , c[1,0] + c[2,0] + c[1,2] + c[2,1] + c[0,2] + c[0,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# # Notes About sklearn vs my classifier:\n",
    "'## I ran the sklearn classifier three times and got resuls [456, 37], [463, 30], and [459, 34]. These results are just slightly worse than the results I got from my classifier: [464, 29], [466, 27], and [463, 30]\n",
    "'## I then changed the training-testing division to 80-20 for the sklearn function. I got results [910, 75], [902, 83], and [918, 67]. These results are comparable to the results of the 90-10 split.   \n",
    "'## The split of correct/incorrect matches is consistent each time I run the classifier: both for mine and the sklearn functions\n",
    "'## I did notice the sklear classifier runs much faster than the one I built. \n",
    "\n",
    "'## Notes About 3 classes w Vibrato:\n",
    "'## With three classes, the results are less strong. I got results [674, 84] which is 88%, [672, 86] which is 89%, and [660,98] which is 87% for an average of 88%. Looking back at my results for two classes using sklear, the results averaged around 92%. So, with three classes the results were around 4% less strong, because there is more room for error. In my confusion matrix, the most common error by a factor of 2 was I got a lot of predicted 'sing' classification when it was in fact true 'vibrato.'\n",
    "'##\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
